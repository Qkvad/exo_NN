%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside]{article}

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[english]{babel} % Language hyphenation and typographical rules

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\roman{subsection}} % roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{Search for Exoplanets $\bullet$ April 2018} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{multicol}
\usepackage{graphicx}
\graphicspath{{img/}}
\usepackage{float}

\newcommand{\code}[1]{\texttt{#1}}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting
\title{Search for Exoplanets} % Article title

\author{%
\textsc{Petra Br\v{c}i\'c}\\% \\[1ex] 
\normalsize Applied Mathematics \\
\normalsize \href{mailto:petrabrcic94@gmail.com}{petrabrcic94@gmail.com} 
\and
\textsc{Sandro Lovni\v{c}ki}\\% \\[1ex] 
\normalsize Computer Science \\ 
\normalsize \href{mailto:lovnicki.sandro@gmail.com}{lovnicki.sandro@gmail.com}
}

\date{\today} 
\renewcommand{\maketitlehookd}{%
\noindent \textbf{\\Abstract:}
With the launch of Kepler space observatory in March 2009, the search for exoplanets has extensively began. There are currently $3,758$ confirmed exoplanets and contrary to the early approaches of validating exoplanet candidates by hand, machine learning has begun taking its toll on the subject. We investigate a convolutional neural network model for detecting transiting exoplanets from Kepler observatory light-curve data collected over a period of $4$ years, with over $150,000$ documented stars. On April 18, 2018, a new satellite (TESS) has been launched, specificaly designed to search for exoplanets using the transit method. After it's 2-year planned mission, complete data should be available to public and more than $20,000$ new exoplanets are expected to be found.
\\\\
\textbf{Keywords:} Kepler, light-curve, exoplanet, machine learning, convolutional neural network.
}


%----------------------------------------------------------------------------------------
\begin{document}

% Print the title and table of contents
\maketitle
\tableofcontents
\newpage

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------
\begin{multicols}{2}

\section{Introduction}
We investigated an implementation of convolutional neural network AstroNet proposed by Shallue and Vanderburg, built with TensorFlow. Using it's high level syntax, we dug deeper into workings of TensorFlow as well as AstroNet, finally building our own classification model with many additional useful inner logs (like mid-evaluation filter values plotting and convolutional layers architecture analysis).

In this paper, we will be talking about processing the raw Kepler light-curves so it can be fed to the neural network in a more meaningful form and overall architecture, learning process and power of our convolutional neural network model. Lastly, we present some interesting results we encountered during this search for a new home.

%------------------------------------------------
\section{Data}
Data used for this project is obtained by combining NASA Exoplanet Archive data for Threshold-Crossing Events which (currently) consists of $34032$ documented TCEs, each having $155$ column features and raw Kepler light-curves (star's flux (brightness) over time) at Mikulski Archive for Space Telescopes consisting of over $150,000$ star light-curves monitored by Kepler.

Most important TCE features for our projects are \code{kepid}, \code{tce\_period}, \code{tce\_time0bk}, \code{tce\_duration} and \code{av\_training\_set} which (respectively) correspond to the Kepler identifier of host star, period of the event, time of the first appearance (more precisely; time of the beginning of drop $+$ \code{tce\_duration}$/2$), duration of the event (drop in star's brightness) and label from set $\{PC,NTP,AFP,UNK\}$ meaning planet candidate, non-transit phenomena, astrophysical false positive and unknown. Those features will enable us to download just the Kepler light-curves of interest and later on to process each light-curve to extract and amplify features of TCE.

\subsection{Creating Dataset}
At the time of our data collecting, and older version of a TCE table was available with $20367$ labeled TCEs. Distribution of \code{av\_training\_set} labels is as follows:
\begin{itemize}
	\item $3600$ TCEs with label PC (planet candidate)
    \item $2541$ TCEs with label NTP (non-transit phenomena)
    \item $9596$ TCEs with label AFP (astrophisical false positive)
    \item $4630$ TCEs with label UNK (unknown)
\end{itemize}
We used that table, specifically \code{kepid} colum to download just the light-curves for those stars from Kepler data. These light-curves are taking about $90$GB of space and are just a fraction of all the Kepler data.

Still, our machines are not fully prepared to tackle the preprocessing and learning from such a vast amount of not at all simple data, so we decided to choose randomly just some of the TCEs from table. We ended up selecting $1058$ TCEs labeled as planet candidate, $471$ TCEs labeled as astrophisical false positive and $734$ TCEs labeled as non transit phenomena.

As an example, here is a small table of selected features of one TCE documented for a star with \code{kepid} $5088591$:
\begin{center}
  \begin{tabular}{||c | c||} 
    \hline
    kepid & 9517393 \\
    \hline
    tce\_period & 219.322 \\
    \hline
    tce\_time0bk & 320.728 \\
    \hline
    tce\_duration & 0.5125000000000001 \\
    \hline
    av\_training\_set & PC \\
    \hline
    \ldots & \ldots \\
    \hline
    tce\_plnt\_num & 1 \\
    \hline
    tce\_eqt & 313.0 \\
    \hline
    \ldots & \ldots
  \end{tabular}
\end{center}

\subsection{Preprocessing Lightcurves}
In order to feed our model, we first need to process the raw light-curve into more useful and compact form. Raw light-curve is loaded as an array of smaller arrays called quarters which represent distinct periods in Kelper telescope's configuration, position, etc. We thus might expect quarters to be on a slightly different scale. In figure~\ref{fig:raw_lc} we show lightcurve for \code{kepid} $5088591$ right after loading \code{time} and \code{flux} arrays.
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{rawLC-9517393}
\caption{Raw light-curve data}
\label{fig:raw_lc}
\end{figure}

It is easy to see that almost every star has an unpredictable variation in its flux, so the data is pretty distorted and irregular which is something that is to be avoided because it would interfere with our model's conclusions and feature extraction. Thus, we calculate a B-spline over the entire light-curve data, ignoring points that cannot be fitted or are of type \code{None}. This spline can be seen in figure!\ref{fig:raw_spline_lc} colored black.
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{splineLC-9517393}
\caption{Raw light-curve data with fitted B-spline}
\label{fig:raw_spline_lc}
\end{figure}

Now we need to exactly find the position of desired TCE which we know from TCE table, column \code{tce\_time0bk}. Focusing just on that segment of this plot, we get figure~\ref{fig:enlarged_tce}
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{splinezoomLC-9517393}
\caption{Enlarged area of the TCE}
\label{fig:enlarged_tce}
\end{figure}

Next, we divide light-curve by the spline to obtain~\ref{fig:divided_lc}. Notice how specific TCEs can now be seen without even enlarging. 
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{divsplineLC-9517393}
\caption{Raw light-curve data divided by spline}
\label{fig:divided_lc}
\end{figure}

We see that the amount of useful data of this "drop" is very poor, but we must remember that those events are periodic by definition so we hope that there is more instances of that same "planet" crossing during the lifetime of Kepler. We find the \code{tce\_period} from TCE table and fold every drop separated by \code{tce\_period} to \code{time0bk}. Now we divide TCE's duration into $201$ discrete points and calculate median value for flux in each of $200$ intervals in between. This results in a much richer, and uniform for all events, representation of a TCE which can be seen in!\ref{fig:folded_tce}. We call that view a "local view" and it is going to be the input of our model.
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{localvierLCdrop-9517393}
\caption{Folded local view of the TCE}
\label{fig:folded_tce}
\end{figure}

%------------------------------------------------
\section{Convolutional Neural Network}
With the recent rise of neural networks, many variations are being explored for specific problems. In classification, convolutional neural networks have been proven to yield best results and we will be using one in this project.  Let's first briefly introduce the reader with the concept of convolutional neural networks and how exactly they differ from regular neural networks.

A convolutional neural network (CNN) can be viewed as a deep neural network with addition of feature extraction mechanism in the from, that is --- before the classification with fully connected layers begin. Building block of convolutional neural network are convolutional layers with pooling and fully connected layers for classification. General scheme of one CNN with two layers of 2D convolutions can be seen in figure~\ref{fig:cnn}.
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{cnn}
\caption{Scheme of a 2D CNN}
\label{fig:cnn}
\end{figure}

In previous figure, we have a CNN that is used for classifying 2D data (generally, images are third order tensors, but we can assume image to be grayscale). Our data is not two-dimensional so we will be using 1D convolution with ReLu function as activation and maximum pooling, each of which will be explained in the following subsections, specifically as they were in out project.

The full graph of our convolutional neural network viewed in TensorBoard can be seen in figure~\ref{fig:exo_tfgraph}.
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{exo_nn_tfgraph}
\caption{TensorFlow graph of our exo\_nn convolutional neural network}
\label{fig:exo_tfgraph}
\end{figure}

\subsection{Convolutional Layer}
A convolutional layer consists of neurons that are not fully connected, that is --- each neurons in this layer is connect to just the small portion of the input. The convolution layer’s parameters consist of a set of learnable filters. During the forward pass, we slide (more precisely, convolve) each filter across the input and compute dot products between the entries of the filter and the input at any position. As we slide the filter over the the input, we will produce a $1$-dimensional activation map that gives the responses of that filter at every position. Furthermore, we can stack multiple convolutional layers before pooling their output. One thing to notice here is that there are two options in doing the described procedure; with padding the input and without padding it, meaning that with padding we add zero values to the left or right of the input so the convolution step on the first/last few inputs in the input array will be valid (there will be enough inputs on each side to do the dot product). We use padding, thus are able to obtain same-size output after the convolution. The convolutional kernel size we use is $5$.

We have a set of $16$ filters in first convolution block and each of them will produce a separate $1$-dimensional activation map. We stack these activation maps along the depth dimension and produce the output. So, for out input representation --- arrays of length $201$ (shape $(?,201)$, where question mark is batch size during training and $1$ during prediction), the output of convolution layer will be of shape $(?,201,16)$. We then feed this data into another convolutional layer with a set of $16$ filters yielding as outputs tensors of shape $(?,201,16)$.

A visualization of what each of $16$ filters is doing with the input 1D array of data can be seen in gifure~\ref{fig:1d_conv} 
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{1d_conv}
\caption{Visualization of 1D convolution}
\label{fig:1d_conv}
\end{figure}

Also, there are $32$ filter in the second convolutional block taking as input the output of pooling the outputs of the first convolutional block. The output of this second convolutional layer is the set of features our convolutional blocks have learned and it is then reduced more with maximum pooling, flattened to shape $(?,1472) $and passed to fully connected layers for classification.

\subsection{Rectified Linear Unit}
For our activation function, we used rectified linear unit (ReLu). It is applied to the dot product of filter and current area the neuron is "looking" at, to produce an output value. One other valid choice for this task would be the sigmoid, but it would not stress enough the importance of larger values.

To be specific, when our network is trained for $1000$ steps with sigmoid activation function, it only manages to get accuracy of $0.5265$ while with ReLu it goes to $0.9735$ in the same number of steps. Furthermore, sigmoid model has $0$ true positives in the confusion matrix and we present why exactly that is so in the next figure.
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{sigmoid_pooling2}
\caption{Output of 32 filters after last pooling before FC (using sigmoid activations)}
\label{fig:sigmoid-pool2}
\end{figure}

As we can see in figure~\ref{fig:sigmoid-pool2}, almost no features are detected and therefore almost every example will be classified as non-transiting phenomena. Notice how vertical scale is pretty much the same and this is because of pretty much the same outputs from sigmoid. We also present the same picture for ReLu.
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{relu_pooling2}
\caption{CHANGE IT Output of 32 filters after last pooling before FC (using ReLu activations)}
\label{fig:relu-pool2}
\end{figure}

\subsection{Max Pooling}
Pooling is a method of downsizing the input data, but keeping the structure. It is used to abstract features obtained by convolutional layer into more complex features that then go further into neural network. We used max pooling with size $7$ and stride $2$ which lead to reducing the output size of the first convolutional block from $201$ to $98$ and output size from the second convolutional layer from $98$ to $46$. Max pooling is done almost like convolution, but we do not calculate the dot-product of "pool filter" of length $7$ with the part of the input of length $7$. Instead, we just take maximum value of $7$ input values we are looking at, slide $2$ steps, collect a maximum value again and so on.

A visualization of what the max pooling is doing to the 1D array of input data can be seen in figure~\ref{fig:max_pooling}
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{max_pooling}
\caption{Visualization of max pooling}
\label{fig:max_pooling}
\end{figure}

\subsection{Fully Connected}
After our second convolutional block, we ended up with $32$ values of length $46$ which are now flattened to form a $1472$ inputs for a network of $2$ fully connected layers with $1024$ neurons each. As activation function, we set ReLu.

We also tried with $4$ layers with $512$ neurons in a layer (and many other combinations) which produced similar or worse accuracy.

A visualization of our fully connected part of the convolutional neural network can be seen in figure~\ref{fig:cnn_fc}
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{cnn_fc}
\caption{Visualization of our fully connected layers at the end of CNN}
\label{fig:cnn_fc}
\end{figure}

\subsection{Sigmoid Output}
Lastly, the computations of fully connected layers are sumarized in a single output value representing the probability that a TCE is a planet candidate. Therefore, we use a sigmoid function as activation for this last neuron.


%-----------------------------------------------
\section{Training}
We train a convolutional neural network model on a randomly constructed set of data consisting of $1058$ planet candidates, $471$ astrophysical false positives and $734$ non-transiting phenomena, summing to $2263$ data examples. For this model (convolutional neural network described above), astrophysical false positives and non-transiting phenomena are both mapped to label $0$ and planet candidates are labeled $1$. 

\subsection{Parameters}
We shuffle the data and partition it into a training set consisting of $1810$ examples, a validation set of $226$ examples and a test set of $227$ examples. Training is done with batch size of $16$, in two main parts; first we train for $3000$ steps with learning rate $10^{-5}$, and then we try to fine tune the accuracy with learning rate $10^{-6}$ for another $1000$ steps. Optimization in both parts is done with Adam optimizer.

\subsection{Progress}
The training process for parameters described above can be most easily viewed through TensorBoard interface for visualizing computation graph and its execution. 
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{train-test_acc}
\caption{Accuracy and Area Under Curve of our model during training and testing, viewed from TensorBoard}
\label{fig:accuracy}
\end{figure}

The accuracy starts stagnating at around $0.9735$ and it seems like we can not get rid of some of the false predictions. View of the values of the confusion matrix during training steps and test can be seen in figure~\ref{fig:confmat}.
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{train-test_cm}
\caption{Confussion matrix values during training and testing}
\label{fig:confmat}
\end{figure}

To conclude, after $7000$ steps we obtained an accuracy of $0.9734513$ with $220$ correctly classified examples in the validation set ($6$ wrongly; $2$ false negatives and $4$ false positives). Total training time was about $16$ minutes and $20$ seconds, on a very slow machine.

\subsection{Testing}
Testing results can also be seen in figures ~\ref{fig:accuracy} and ~\ref{fig:confmat} and are represented with a red dot. Explicitly, we obtained a $0.9295154$ accuracy on the training set with $211$ correctly classified examples ($16$ wrongly; $4$ false negatives and $12$ false positives).

It is expected that there are more false positives because they are tricky to find even for professionals with specialized equipment.


%----------------------------------------------
\section{Results}
In this last section, we present some of the most interesting results we encountered during our journey of training and testing our exoplanet classifier.\\

Lets first observe PCs. We observed quite a lot of raw light curves to find the ones most interesting to observe more carefully and present in this work. The star with the \code{kepid} $10004738$ has two potential planets orbiting around it. The first one has quite significant drop in the light-curve that can be seen in figure~\ref{fig:resPC1}, something even more catchy is the fact that its period is as the period of Mercury, and the other planet, which drop in the light-curve can be seen in figure~\ref{fig:resPC11}, has a period just a litte more that half of its neighbouring planet. 
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{resPC1}
\caption{Exoplanet with \code{kepid} $10004738$ and \code{period} $92.8747$}
\label{fig:resPC1}
\end{figure}

After running the code to check the prediction on both these planet candidates, we obtained quite astonishing results of $0.9564$ and $0.9166$ respectively to each candidate mentioned.
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{resPC11}
\caption{Exoplanet with \code{kepid} $10004738$ and \code{period} $56.4757$}
\label{fig:resPC11}
\end{figure}

There is one more PC we would like to comment on, it is revolving aorund the star with the \code{kepid} $3656121$, its prediction is $0.9563$ and its drop in the light-curve can be seen in figure~\ref{fig:resPC2}.
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{resPC2}
\caption{Exoplanet with \code{kepid} $3656121$ and \code{period} $31.1588$}
\label{fig:resPC2}
\end{figure}
Some PCs that didn't make the cut, but are worth mentioning are with \code{kepid} $8292840$ with the prediction of $0.9384$ and the one with \code{kepid} $11391018$ with the prediction of $0.73$. We recommend to the reader to observe some of the data on their own, maybe you find new exoplanet and get the acknoledgement from NASA. 

Let's now observe the results obtained for unknows (UNK), astrophysical false positive (AFP) and non transit phenomenon (NTP).  
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{resUNK1}
\caption{Unknown with \code{kepid} $5094751$ with the prediction $0.8843$}
\label{fig:resUNK1}
\end{figure}

\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{resUNK2}
\caption{Unknown with \code{kepid} $8229048$ with the prediction $0.1898$}
\label{fig:resUNK2}
\end{figure}

\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{resAFP}
\caption{AFP with \code{kepid} $4544571$ with the prediction $0.007$}
\label{fig:resAFP}
\end{figure}

\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{resNTP}
\caption{NTP with \code{kepid} $9696853$ with the prediction $0.0000003$}
\label{fig:resAFP}
\end{figure}

Results on all of the last five figures were as low as expected with one exception, which is the reason why we included it in this document, in figure~\ref{fig:resUNK1} with quite high prediction. But this unkown is not alone in this system. After doing some research we found out that the star with \code{kepid} $5094751$ is acctualy known as Kepler-109 and it has two planet canindates named Kepler-109 b and Kepler-109 c which additional information can be seen in \cite{oExoCat}. Based on the prediction obtained for the unknown candidate in planetary system around Kepler-109, it might be the case that it now holds, not two but three planets. 
\\
We are planning to continue our search for exoplanets as long as we can because we both share passion for astronomy. We will improve our model and preprocess part of the code so we extract the most that we can from the data as fast as we can. New data from TESS will arive and we will be ready for it. 
%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\begin{thebibliography}{99}

\bibitem[1]{kepler:wiki}
\url{https://en.wikipedia.org/wiki/Kepler_(spacecraft)}

\bibitem[2]{tess:wiki}
\url{https://en.wikipedia.org/wiki/Transiting_Exoplanet_Survey_Satellite}

\bibitem[TESS]{tess}
\url{https://tess.mit.edu/}

\bibitem[3]{exo:methods}
\url{https://en.wikipedia.org/wiki/Methods_of_detecting_exoplanets}

\bibitem[4]{microlensing:exos}
\url{https://en.wikipedia.org/wiki/List_of_exoplanets_detected_by_microlensing}

\bibitem[5]{radial:exos}
\url{https://en.wikipedia.org/wiki/List_of_exoplanets_detected_by_radial_velocity}

\bibitem[6]{kepler:exos}
\url{https://en.wikipedia.org/wiki/List_of_exoplanets_discovered_using_the_Kepler_spacecraft}

\bibitem[Shallue and Vanderburg, 2017]{Shallue:2017}
Christopher J. Shallue, Andrew Vanderburg (2017).
\newblock Identifying Exoplanets with Deep Learning: A Five Planet Resonant Chain around Kepler-80 and an Eighth Planet around Kepler-90
\newblock arXiv:1712.05044 [astro-ph.EP]

\bibitem[Mikulski Archive for Space Telescopes]{mikulski}
\url{https://archive.stsci.edu/}

\bibitem[NASA Exoplanet Archive]{NASA:exos}
\url{https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=q1_q17_dr24_tce}

\bibitem[TCEs table]{tces}
\url{https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=q1_q17_dr24_tce}
 
\bibitem[Open Exoplanet Catalogue]{oExoCat}
\url{http://www.openexoplanetcatalogue.com/planet/Kepler-109%20b/} 
\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{multicols}{2}
\end{document}
